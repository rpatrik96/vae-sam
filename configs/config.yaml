# pytorch_lightning==1.8.0.post1
seed_everything: true
trainer:
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      log_model: true
      project: sam_test
      save_dir: vae_sam
  enable_checkpointing: true
  enable_progress_bar: true
  overfit_batches: 0.0
  track_grad_norm: -1
  check_val_every_n_epoch: 15
  fast_dev_run: false
  max_epochs: null
  max_steps: -1
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  log_every_n_steps: 50
  precision: 32
  detect_anomaly: true
model:
  input_height: 32
  enc_type: resnet18
  first_conv: false
  maxpool1: false
  enc_out_dim: 512
  kl_coeff: 0.1
  latent_dim: 256
  lr: 0.0001
  rho: 1.0
  sam_update: false
  norm_p: 2.0
  offline: true
data:
  data_dir: ./data/cifar10
  val_split: 0.2
  num_workers: 0
  normalize: false
  batch_size: 32
  seed: 42
  shuffle: true
  pin_memory: true
  drop_last: false
  train_transforms: null
  val_transforms: null
  test_transforms: null
notes: null
tags: null
early_stopping:
  monitor: val_loss
  min_delta: 0.0
  patience: 5
  verbose: false
  mode: min
  strict: true
  check_finite: true
  stopping_threshold: null
  divergence_threshold: null
  check_on_train_epoch_end: null
  log_rank_zero_only: false
ckpt_path: null
